
trial: 0  # id for recording multiple runs
do_eval: False
device_id: [0]
training_dataset: meld  

cwd: ${hydra:runtime.cwd}
seed: 1111
data:
  num_labels: 7
  audio_feature_dim: 768
  vision_feature_dim: 512  
  audio_utt_max_len_pkl: 132
  audio_utt_max_len: 168   # 1249: ivalue-len:400000; 168: ivalue-len:53861; 624: ivalue-len: 200000
  context_max_len: 256  
  context_pad_value: 1    
  text_utt_max_len: 38     # max len in one utterance
  vision_utt_max_len: 100
  transform:
    mean: [ 0.5, 0.5, 0.5 ]
    std: [ 0.5, 0.5, 0.5 ]
    color_jitter:
      brightness: 0.5
      contrast: 0.5
      saturation: 0.5
      hue: 0.5
    resize:
      target_size: 160

dataset:
  anno_csv_path: ${cwd}/../common/data/meld
  data_load_path: ${dataset.anno_csv_path}/preprocessed_data
  text_path: ${dataset.data_load_path}/text
  meld:
    anno_csv_path: ${cwd}/../common/data/meld
    video_dir: ${dataset.anno_csv_path}/raw/MELD.Raw
    emotion_vocab_path: ${dataset.anno_csv_path}/meld_vocab.pkl
    # audio_ivalues_path: ${dataset.data_load_path}/audio/penny_meld_audio_ivalues.pkl   # values from audio processor
    # audio_ivalues_path_wav2vec: ${dataset.data_load_path}/audio/penny_meld_audio_ivalues_wav2vec.pkl
   
model:
  text_encoder:
    # use_text_teacher: False
    # use_text_teacher_for_audio: False
    # use_frozen_teacher: False
    pretrained_path: 'princeton-nlp/sup-simcse-roberta-large'
    embed_dim: 1024
    pad_value: 1
    mask_value: 2
  
  vision_encoder:
    model_name: 'inceptionresnetv1' # or resnet50
    use_webface_pretrain: True
    # use_imgnet_pretrain: False  # for resnet50
    # pretrained_path: '' # '/home/penny/pycharmprojects/common/models/pretrained_model/ckpt_epoch_134.pth'
  
  # audio_encoder:
  #   model_name: 'data2vec'  # or wave2vec
  #   pretrained_path: 'facebook/data2vec-audio-base-960h'   #default, used in TelME
  #   pretrained_path_wav2vec: 'facebook/wav2vec2-base-960h'
    # pretrained_path: ['facebook/wav2vec2-large-960h', 'facebook/data2vec-audio-base-960h'] # 'facebook/data2vec-audio-base-960h' (TelME)
  



  transformers:
    # openface_feat_attn:
    #   num_transformer_layers: 2
    #   num_attn_heads: 12
    #   intermediate_size: 128
    #   hidden_activation: gelu
    #   hidden_dropout_prob: 0.1
    #   attn_probs_dropout_prob: 0.1
    #   layer_norm_eps: 1e-12
    #   initializer_range: 0.02

    hidden_size: 768
    self_attn_transformer:
      num_transformer_layers:
        # audio: 5
        vision: 2
#      hidden_size: 768
      num_attn_heads: 12
      intermediate_size: 3072
      hidden_activation: gelu
      hidden_dropout_prob: 0.1
      attn_probs_dropout_prob: 0.1
      layer_norm_eps: 1e-12
      initializer_range: 0.02

    cross_modal_transformer:
      text_vision:
        num_transformer_layers: 2
        num_attn_heads: 12
        attn_dropout: 0.1
      # text_audio:
      #   num_transformer_layers: 2
      #   num_attn_heads: 12
      #   attn_dropout: 0.1
      # text_audio_vision:
      #   num_transformer_layers: 2
      #   num_attn_heads: 12
      #   attn_dropout: 0.1

train:
  vle_model_name: vle_model.pth 
  # tv_model_name: kd_teacher_tv.pth
  # kd:
  #   tstudent_check_clarity: False   # check teacher clarity
  #   alpha: 0.01
  #   beta: 0.5
  #   fusion_frozen_s: True  # frozen students while training fusion module
  # kd_student: 1    # 1 for vision, 0 for audio
  # print_error_idx: False
  # use_focal_loss: False  # whether use focal loss or not
  # use_weighted_ce: False 
  # label_guidance: False  # concat with true labels to test
  # cosine: True  # use cosine decay scheduler
  # df_lr: False   # use different lr for different modules
  save_model: False
  # apply_cross_attn_mask: False
  # tfeat_mask_min: 0.3
  # tfeat_mask_by_teacher: False   # mask text token by confidence of teacher model
  # vfeat_orth_loss_ratio: 0.01
  # vfeat_group_orth: False  # orth loss between groups
  # vfeat_use_text_feat: False
  # vfeat_teacher_confidence_min: 0.6
  # vfeat_orthogonal: False
  vfeat_neutral_norm: False 
  # vfeat_apply_au: False  # apply action unit 
  # vfeat_append_au: False  # append action unit value 
  # vfeat_one_frame: False  # select one frame (in combination with kf 4/44) 1.max_sim, 2. sequential, 3.random
  # afeat_from_pkl: True       # if true, load preexracted audio feature from pkl
  # afeat_penny: False   # use features extracted by penny
  # vfeat_filter_masked_openface: False  # mainly for openface masked images
  # vfeat_penny: False     # use features extracted by penny
  # vfeat_zeros: False   # zero out all facial expressions
  # vfeat_keyframes: 0   # 1.apply keyframes before vision transformer (wrt text embeddings), 11. scores<0.5, 2. apply after (wrt text embeddings), 22. scores<0.5, 3. wrt prototype vectors.
 # 4. vision--audio before self-attn (44. zero out vision embeddings) 5. vision--audio after self attn (55. zero out vision embedding)
  # vfeat_random_group: False
  # vfeat_larger_group: False
  # vfeat_kf_threshold: 0.5 
  # vfeat_filter2: 0 # filter the image frames to the configured number
  # vfeat_supv_detach: True  # detach other expert embedding 
  # vfeat_detach_for_score: False
  # vfeat_truncate: 0 # 100
  # vfeat_downsample_to: 0  # downsampling to {} frames
  # vfeat_truncate_r: False   # truncate from end
  # vfeat_from_pkl: True      # if True, load preextracted vision feature from pkl
  # early_stop_check_epochs: -1   # if best val wf not updated for {} epochs, then stop training
  # max_train_steps: -1     # max training steps per epoch
  # multimodal_fusion: 0   # 1 for ASF, 2 for MAG, 0 for CMA
  # weight_init: 1  # apply customized weight init
  num_workers: 16
  resnet_trainable: True
  # audio_encoder_trainable: False
  # audio_mask: False   # whether to apply audio_mask 
  # spcl_loss:
  #   temperature: 0.08
  #   pool_size: 512
  #   support_set_size: 64
  # loss_fn: ''
  num_epochs: 20
  batch_size: 16
  accumulation_steps: 2
  lr: 4e-7 # 4.5e-7
  weight_decay: 0.05
  warm_up: 0.02 # 0.1
  gradient_clip_value: 10   # 5: ref to SPCL , 10: ref to TelME , 0.8: ref to FacialMMT
  log_interval: 100
  # aux_log_interval: 100
  save_model_path: ${cwd}/saved_model

# eval:
#   model_path:
#     root: ${cwd}/pretrained_model/FacialMMT-RoBERTa
#   #  swin: ${cwd}/saved_model/best_swin_04-21-00-24-10.pt
#     multimodal: ${cwd}/saved_model/multimodal_model_T+A+V_05-02-19-36-56.pt
#     # swin: ${eval.model_path.root}/best_swin_RoBERTa.pt
#     # multimodal: ${eval.model_path.root}/multimodal_T+A+V_RoBERTa.pt








